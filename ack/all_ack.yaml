# Consolidated ACK file
# This file contains all acknowledgments from individual version/test-type files
# Each entry includes optional 'version' and 'test' metadata for filtering
# Format: {version}_{test-type}_ack.yaml

---
ack:

  # === Version 4.22 ===

  # cluster-density
  - uuid: 5c3c9245-b9e1-4849-86c8-34c31d81898a
    metric: multusCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.22'
    test: cluster-density-v2
  - uuid: 5c3c9245-b9e1-4849-86c8-34c31d81898a
    metric: ovnCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.22'
    test: cluster-density-v2
  - uuid: fe336609-b83f-4dc4-a878-d68f5e25018a
    metric: ovnCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.22'
    test: cluster-density-v2
  - uuid: e023e975-ad9f-404a-907f-b0756d3f7473
    metric: multusCPU_avg
    reason: cdv2 updated metrics profile https://github.com/kube-burner/kube-burner-ocp/pull/380
    version: '4.22'
    test: cluster-density-v2
  - uuid: e023e975-ad9f-404a-907f-b0756d3f7473
    metric: ovnCPU_avg
    reason: cdv2 updated metrics profile https://github.com/kube-burner/kube-burner-ocp/pull/380
    version: '4.22'
    test: cluster-density-v2
  # crd-scale
  - uuid: 326e738e-bbd5-4398-84c9-2d74b44c3758
    metric: apiserverCPU_avg
    reason: Random dip and spike for CPU
    version: '4.22'
    test: crd-scale
  # node-density
  - uuid: 679b905a-318c-4b8c-a630-4b2dcc394d25
    metric: ovnCPU_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.22'
    test: node-density
  - uuid: 679b905a-318c-4b8c-a630-4b2dcc394d25
    metric: kubelet_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.22'
    test: node-density
  # node-density-cni
  - uuid: ade4d0f9-84d8-4edf-926a-4aa88ca14f61
    metric: ovnCPU-ovncontroller_avg
    reason: Random CPU spike
    version: '4.22'
    test: node-density-cni
  - uuid: fdbe9f2c-6408-4c8f-82aa-aecccd0029ea
    metric: ovnCPU-ovncontroller_avg
    reason: Old CPU spike
    version: '4.22'
    test: node-density-cni
  - uuid: 6ca8825c-3a47-4f0f-9cfe-b4a45986bd37
    metric: podReadyLatency_P99
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.22'
    test: node-density-cni
  - uuid: 6ca8825c-3a47-4f0f-9cfe-b4a45986bd37
    metric: ovsCPU-Workers_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.22'
    test: node-density-cni
  - uuid: 6ca8825c-3a47-4f0f-9cfe-b4a45986bd37
    metric: ovnMem-ovncontroller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.22'
    test: node-density-cni
  - uuid: f7567913-9543-444c-9e1d-2203ce467696
    metric: kubelet_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.22'
    test: node-density-cni
  - uuid: 6ca8825c-3a47-4f0f-9cfe-b4a45986bd37
    metric: kubelet_avg
    reason: EXTRA_FLAGS issue wrt node-density-cni test
    version: '4.22'
    test: node-density-cni

  # === Version 4.21 ===

  # cluster-density
  - uuid: 12414e33-883e-4ef1-91b3-cac7b0cce3c5
    metric: multusCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.21'
    test: cluster-density-v2
  - uuid: 12414e33-883e-4ef1-91b3-cac7b0cce3c5
    metric: ovnCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.21'
    test: cluster-density-v2
  # node-density
  - uuid: 540fb223-bc18-4565-874d-79a5443d7003
    metric: apiserverCPU_avg
    reason: apiserver increase from 4.19 to 4.21, no obvious reason, no code changes,
      rhcos didn't change either
    version: '4.21'
    test: node-density
  - uuid: ad775826-d6bf-4017-8efe-3350602868c5
    metric: podReadyLatency_P99
    reason: podReadyLatency_P99 fluctuates between 2s and 3s, no obvious reason
    version: '4.21'
    test: node-density
  - uuid: ad49121c-0151-4479-9f30-7809473fe9e9
    metric: apiserverCPU_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.21'
    test: node-density
  - uuid: ca5bf944-1229-4b12-8296-c975be0f0fb4
    metric: ovnCPU_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.21'
    test: node-density
  - uuid: ca5bf944-1229-4b12-8296-c975be0f0fb4
    metric: kubelet_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.21'
    test: node-density
  # node-density-cni
  - uuid: 5bdef865-13ce-40fb-94be-4117ecdf8f4d
    metric: ovnCPU-ovnk-controller_avg
    reason: It has come back down in later runs
    version: '4.21'
    test: node-density-cni
  - uuid: 964c3ddb-9272-4693-9cf2-c749e11f1d8d
    metric: podReadyLatency_P99
    reason: It has come back down in later runs
    version: '4.21'
    test: node-density-cni
  - uuid: b80c808a-04e0-4fc5-ab06-2a5a10ebe0c8
    metric: multusCPU_avg
    reason: Multus CPU jumps around throughout release
    version: '4.21'
    test: node-density-cni
  - uuid: 10c07387-55e9-4721-a5c8-8ab006297be1
    metric: ovnCPU-ovncontroller_avg
    reason: ovnk-controller cpu spike (random)
    version: '4.21'
    test: node-density-cni
  - uuid: 10c07387-55e9-4721-a5c8-8ab006297be1
    metric: ovnkCPU-overall_avg
    reason: old CPU spike
    version: '4.21'
    test: node-density-cni
  - uuid: 10c07387-55e9-4721-a5c8-8ab006297be1
    metric: ovnCPU-ovncontroller_avg
    reason: old CPU spike
    version: '4.21'
    test: node-density-cni
  - uuid: 10c07387-55e9-4721-a5c8-8ab006297be1
    metric: ovnCPU-ovnk-controller_avg
    reason: old CPU spike
    version: '4.21'
    test: node-density-cni
  - uuid: 3a6b3569-356a-4895-9877-4b722ca0844f
    metric: kubelet_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.21'
    test: node-density-cni
  - uuid: 3a6b3569-356a-4895-9877-4b722ca0844f
    metric: ovsCPU-Workers_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.21'
    test: node-density-cni
  - uuid: 3a6b3569-356a-4895-9877-4b722ca0844f
    metric: ovnMem-ovncontroller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.21'
    test: node-density-cni
  - uuid: 539402f9-9dd1-435e-9128-ae7e6417a047
    metric: podReadyLatency_P99
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.21'
    test: node-density-cni
  # udn-l3
  - uuid: 80e3e90d-d974-49fa-8e39-3e44afcbb408
    metric: ovsCPU_avg
    reason: same nightly
    version: '4.21'
    test: udn-l3
  - uuid: 80e3e90d-d974-49fa-8e39-3e44afcbb408
    metric: ovsMemory_avg
    reason: same nightly
    version: '4.21'
    test: udn-l3
  - uuid: 80e3e90d-d974-49fa-8e39-3e44afcbb408
    metric: ovnMem-ovncontroller_avg
    reason: same nightly
    version: '4.21'
    test: udn-l3
  - uuid: 80e3e90d-d974-49fa-8e39-3e44afcbb408
    metric: ovnMem-northd_avg
    reason: same nightly
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovsCPU_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovsMemory_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovnMem-ovncontroller_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovnMem-northd_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovnMem-nbdb_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovnMem-sbdb_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3
  - uuid: 60d9fb49-648e-4e26-ab4d-e8704f749e80
    metric: ovnkMem-overall_avg
    reason: shift from w/o pause to with pause in config
    version: '4.21'
    test: udn-l3

  # === Version 4.20 ===

  # cluster-density
  - uuid: a6ee097c-24f8-470f-a2c8-cd21651fd497
    metric: multusCPU_avg
    reason: 2025-04-14 change of 19.9848% is because whereabouts-reconciler pods were
      removed. No CPU change in remaining pods. See PERFSCALE-3872
    version: '4.20'
    test: cluster-density-v2
  - uuid: f1c30ca0-1e88-4a65-9c98-2a47f162c771
    metric: monitoringCPU_avg
    reason: https://issues.redhat.com/browse/PERFSCALE-4043
    version: '4.20'
    test: cluster-density-v2
  - uuid: fb9dd3a10-3c94-4667-8ae4-943a5ba4de13
    metric: multusCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.20'
    test: cluster-density-v2
  - uuid: b9dd3a10-3c94-4667-8ae4-943a5ba4de13
    metric: multusCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.20'
    test: cluster-density-v2
  - uuid: b9dd3a10-3c94-4667-8ae4-943a5ba4de13
    metric: ovnCPU_avg
    reason: Missing data in ES, indexing failures
    version: '4.20'
    test: cluster-density-v2
  # crd-scale
  - uuid: 275567ff-cd8d-4736-b527-afc364c2216f
    metric: etcdCPU_avg
    reason: Possibly related to kube rebase - https://issues.redhat.com/browse/OCPBUGS-59641
    version: '4.20'
    test: crd-scale
  - uuid: 3924177b-afd2-4b38-af47-719fc9352e27
    metric: apiserverCPU_avg
    reason: Possibly related to kube rebase - https://issues.redhat.com/browse/OCPBUGS-59641
    version: '4.20'
    test: crd-scale
  # node-density
  - uuid: 35dbe753-c8c6-4ecf-9070-83e45fede818
    metric: kubelet_avg
    reason: OCPBUG is opened https://issues.redhat.com/browse/OCPBUGS-59641
    version: '4.20'
    test: node-density
  - uuid: 35dbe753-c8c6-4ecf-9070-83e45fede818
    metric: podReadyLatency_P99
    reason: OCPBUG is opened https://issues.redhat.com/browse/OCPBUGS-59641
    version: '4.20'
    test: node-density
  - uuid: ffb0ec28-7d57-4ec3-8359-48b5bd65b502
    metric: kubelet_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.20'
    test: node-density
  # node-density-cni
  - uuid: ff18df52-4d57-4089-8a33-cdb8ed3497c0
    metric: podReadyLatency_P99
    reason: Tracking in thread https://redhat-internal.slack.com/archives/CU9HKBZKJ/p1760097075174859?thread_ts=1759941599.018259&cid=CU9HKBZKJ
    version: '4.20'
    test: node-density-cni
  - uuid: 4a2dc390-ac4d-4c3c-9fe8-db91208f7c96
    metric: podReadyLatency_P99
    reason: podReadyLatency_P99 fluctuates between 4s and 5s
    version: '4.20'
    test: node-density-cni
  - uuid: a5b3a3a7-2e3e-4b93-8931-16ea85e34abb
    metric: ovsMemory-Workers_max
    reason: recovered in subsequent runs
    version: '4.20'
    test: node-density-cni
  - uuid: a5b3a3a7-2e3e-4b93-8931-16ea85e34abb
    metric: ovnMem-ovncontroller_avg
    reason: recovered in subsequent runs
    version: '4.20'
    test: node-density-cni
  - uuid: 6ce3a213-8481-4282-84ce-af2368ba3051
    metric: ovnkCPU-overall_avg
    reason: old cpu spike
    version: '4.20'
    test: node-density-cni
  - uuid: 6ce3a213-8481-4282-84ce-af2368ba3051
    metric: ovnCPU-ovnk-controller_avg
    reason: old cpu spike
    version: '4.20'
    test: node-density-cni
  - uuid: fb6be24f-ab09-4302-87f4-56634145ad9d
    metric: kubelet_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.20'
    test: node-density-cni
  - uuid: fb6be24f-ab09-4302-87f4-56634145ad9d
    metric: ovsCPU-Workers_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-74474
    version: '4.20'
    test: node-density-cni
  # udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: podReadyLatency_P99
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnkMem-overall_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnMem-ovncontroller_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnMem-northd_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnMem-nbdb_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnMem-sbdb_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: fe9521f8-71b8-4ad4-b190-4e03269ebd24
    metric: ovnMem-ovnk-controller_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.20'
    test: udn-l3
  - uuid: 7d80399d-f455-4134-b631-d1712c42413e
    metric: ovnMem-nbdb_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-60650
    version: '4.20'
    test: udn-l3
  - uuid: 5af8de0e-83ab-48d2-b930-1547e1eee6e3
    metric: ovnCPU-nbdb_avg
    reason: job was w/o pause
    version: '4.20'
    test: udn-l3
  - uuid: 5af8de0e-83ab-48d2-b930-1547e1eee6e3
    metric: ovnCPU-sbdb_avg
    reason: job was w/o pause
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovsCPU_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovsMemory_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovnkMem-overall_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovnMem-northd_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovnMem-nbdb_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovnMem-sbdb_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3
  - uuid: c3f2ff7e-3f26-4b56-a29d-189570d7c14c
    metric: ovnMem-ovncontroller_avg
    reason: shift from w/o pause to with pause in config
    version: '4.20'
    test: udn-l3

  # === Version 4.19 ===

  # cluster-density
  - uuid: cfae6e1e-8c47-4628-be52-8705c8c64285
    metric: kubelet_avg
    reason: kube-burner change which increased cpu for kubelet, it was reverted.
    version: '4.19'
    test: cluster-density-v2
  - uuid: a7006f55-489b-4b79-96ed-5d3fd3f050b0
    metric: ovnCPU_avg
    reason: 'Opened Bug: https://issues.redhat.com/browse/OCPBUGS-49613'
    version: '4.19'
    test: cluster-density-v2
  - uuid: 858c1aa1-eb87-4551-b6b8-f432807fff8d
    metric: podReadyLatency_P99
    reason: 'Opened Bug: https://issues.redhat.com/browse/OCPBUGS-50522'
    version: '4.19'
    test: cluster-density-v2
  - uuid: e20ae931-6b18-46a7-b41d-e9a4ae2099d5
    metric: ovnCPU_avg
    reason: 'See: OCPBUGS-50522. Fixed in: 1889163830622162944'
    version: '4.19'
    test: cluster-density-v2
  - uuid: 84322af4-d853-4c95-aa38-f6417159712d
    metric: etcdCPU_avg
    reason: 'hichup: https://redhat-internal.slack.com/archives/C020CKMP6CT/p1741244229397479'
    version: '4.19'
    test: cluster-density-v2
  - uuid: a1fcb44b-c5fc-4070-bc71-8940184b310f
    metric: multusCPU_avg
    reason: 2025-04-04 change of 3.2% is less than 10% toleration threshold
    version: '4.19'
    test: cluster-density-v2
  - uuid: 411a73f2-3213-40df-ac66-c28b2fdc639d
    metric: ovnCPU_avg
    reason: 2025-04-08 change of 4.62436% is less than 10% toleration threshold
    version: '4.19'
    test: cluster-density-v2
  - uuid: fc90b400-ee81-4fd1-afbf-dd1408eab767
    metric: multusCPU_avg
    reason: 2025-04-14 change of 19.9848% is because whereabouts-reconciler pods were
      removed. No CPU change in remaining pods. See PERFSCALE-3872
    version: '4.19'
    test: cluster-density-v2
  # crd-scale
  - uuid: dc6d26a9-ddbb-4c0d-8946-7641578a2eda
    metric: apiserverCPU_avg
    reason: Change less than 5% on a rather volatile metric
    version: '4.19'
    test: crd-scale
  # node-density
  - uuid: fe3c49e3-0b53-4fb1-88bf-bbf3bd187efc
    metric: ovnCPU_avg
    reason: 'Opened Bug: https://issues.redhat.com/browse/OCPBUGS-49613'
    version: '4.19'
    test: node-density
  - uuid: e6db93d9-bcb7-47e8-a4d2-143b3db4fabc
    metric: etcdCPU_avg
    reason: Abnormally high etcdCPU looks like it was fixed at 1890069956565929984
    version: '4.19'
    test: node-density
  - uuid: 0bbf62c5-0f8d-4c09-a7c9-c93f772cd2dd
    metric: apiserverCPU_avg
    reason: apiserver increase from 4.19 to 4.21, no obvious reason, no code changes,
      rhcos didn't change either
    version: '4.19'
    test: node-density
  - uuid: 6da3af3c-fe09-405f-9721-82ffcc03dec1
    metric: ovnMem-ovncontroller_avg
    reason: ''
    version: '4.19'
    test: node-density
  - uuid: bca2670e-588f-4e69-b9b0-1c3df1a8de2f
    metric: apiserverCPU_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.19'
    test: node-density
  - uuid: bca2670e-588f-4e69-b9b0-1c3df1a8de2f
    metric: ovnCPU_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.19'
    test: node-density
  - uuid: bca2670e-588f-4e69-b9b0-1c3df1a8de2f
    metric: kubelet_avg
    reason: CPU bumps due to unintentional change in deletion strategy
    version: '4.19'
    test: node-density
  # node-density-cni
  - uuid: fd0ab680-95f1-4556-8706-4823e5b7ee21
    metric: podReadyLatency_P99
    reason: Performance has returned
    version: '4.19'
    test: node-density-cni
  # udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnkCPU-overall_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnCPU-ovncontroller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnCPU-northd_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnCPU-nbdb_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnCPU-ovnk-controller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnkMem-overall_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnMem-ovncontroller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnMem-northd_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnMem-nbdb_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnMem-sbdb_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: ae5bf2c9-6958-4730-8213-1321e3fb1fec
    metric: ovnMem-ovnk-controller_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-54508, OVNK CPU increase after
      downstream merge of PR 2501
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: podReadyLatency_P99
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnkMem-overall_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnMem-ovncontroller_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnMem-northd_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnMem-nbdb_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnMem-sbdb_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: e27c61f2-6cdb-490d-bb6a-59a34048cfc1
    metric: ovnMem-ovnk-controller_avg
    reason: kube-burner change on how we calculate metrics
    version: '4.19'
    test: udn-l3
  - uuid: cb90d90d-a33b-4639-a640-ace2996a708b
    metric: ovnkMem-overall_avg
    reason: job was w/o pause
    version: '4.19'
    test: udn-l3
  - uuid: 00f7a823-5c56-430b-8e73-d5812d8bfe9f
    metric: ovsCPU_avg
    reason: shift from w/o pause to with pause in config
    version: '4.19'
    test: udn-l3
  - uuid: 00f7a823-5c56-430b-8e73-d5812d8bfe9f
    metric: ovsMemory_avg
    reason: shift from w/o pause to with pause in config
    version: '4.19'
    test: udn-l3
  - uuid: 00f7a823-5c56-430b-8e73-d5812d8bfe9f
    metric: ovnkMem-overall_avg
    reason: shift from w/o pause to with pause in config
    version: '4.19'
    test: udn-l3
  - uuid: 00f7a823-5c56-430b-8e73-d5812d8bfe9f
    metric: ovnMem-northd_avg
    reason: shift from w/o pause to with pause in config
    version: '4.19'
    test: udn-l3
  - uuid: 00f7a823-5c56-430b-8e73-d5812d8bfe9f
    metric: ovnMem-nbdb_avg
    reason: shift from w/o pause to with pause in config
    version: '4.19'
    test: udn-l3
  - uuid: 15c236e7-abff-4595-8e72-06bfd6115e21
    metric: ovnMem-nbdb_avg
    reason: https://issues.redhat.com/browse/OCPBUGS-60650
    version: '4.19'
    test: udn-l3

  # === Version 4.18 ===

  # cluster-density
  - uuid: 7f7337aa-cee3-4a36-b154-a7c48ed1fb75
    metric: etcdCPU_avg
    reason: Below 10%
    version: '4.18'
    test: cluster-density-v2
  - uuid: 4eeee274-53ac-4e75-855f-139dc743606b
    metric: kubelet_avg
    reason: Started thread with node team
    version: '4.18'
    test: cluster-density-v2
  - uuid: 22e90f4e-1c79-4d9d-b2f6-b95a7072738c
    metric: kubelet_avg
    reason: Tooling issue - kubeburner
    version: '4.18'
    test: cluster-density-v2
  - uuid: 22e90f4e-1c79-4d9d-b2f6-b95a7072738c
    metric: ovnCPU_avg
    reason: Tooling issue - kubeburner
    version: '4.18'
    test: cluster-density-v2
  - uuid: 35bbc7f2-195c-43c2-8d43-62142d8bb33a
    metric: ovnCPU_avg
    reason: 'Opened Bug: https://issues.redhat.com/browse/OCPBUGS-49613'
    version: '4.18'
    test: cluster-density-v2
  - uuid: 90e38f9e-9b90-4f8a-af97-8ccbdb3ef800
    metric: ovnCPU_avg
    reason: See OCPBUGS-49613
    version: '4.18'
    test: cluster-density-v2
  - uuid: 90e38f9e-9b90-4f8a-af97-8ccbdb3ef800
    metric: podReadyLatency_P99
    reason: Latency increase overlaps with ovnCPU_avg. See OCPBUGS-49613
    version: '4.18'
    test: cluster-density-v2
  - uuid: 373effda-bcfd-42fe-8ba5-fd539bf2c755
    metric: kubelet_avg
    reason: False positive. Values look fine.
    version: '4.18'
    test: cluster-density-v2
  - uuid: df4b0361-02f9-458d-9d02-57b1d6724983
    metric: podReadyLatency_P99
    reason: 12s podReady is not unusual in 4.18
    version: '4.18'
    test: cluster-density-v2
  - uuid: e3e07bda-179a-4962-bc14-0e40e247d9fa
    metric: etcdCPU_avg
    reason: 2025-04-11 8.10777% change less than 10% threshold
    version: '4.18'
    test: cluster-density-v2
  # node-density
  - uuid: 7f7337aa-cee3-4a36-b154-a7c48ed1fb75
    metric: etcdCPU_avg
    reason: Below 10%
    version: '4.18'
    test: node-density
  - uuid: fc420bd9-fde1-48ad-ab7b-7843c32164b8
    metric: ovnCPU_avg
    reason: 'Opened Bug: https://issues.redhat.com/browse/OCPBUGS-49613'
    version: '4.18'
    test: node-density
  - uuid: ace8d433-0e4a-4758-9e84-6d5c7566a3ec
    metric: ovnCPU_avg
    reason: See OCPBUGS-49613
    version: '4.18'
    test: node-density
  - uuid: ace8d433-0e4a-4758-9e84-6d5c7566a3ec
    metric: podReadyLatency_P99
    reason: See OCPBUGS-49613
    version: '4.18'
    test: node-density
  # node-density-cni
  - uuid: 61262f14-270e-4122-b2f6-d11175e68525
    metric: ovsMemory_avg
    reason: It has come back down in later runs
    version: '4.18'
    test: node-density-cni
